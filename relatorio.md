# Multi-Agent Speaker-Listener Environment  
*A Reinforcement Learning Project*  

## Overview
This project implements a multi-agent reinforcement learning environment based on the Speaker-Listener setup, where two agents collaborate to solve communication and coordination tasks.

Goal:  
Implement a new multi-agent reinforcement learning algorithm that enables the Listener to reach the goal faster than the MATD3 baseline, achieving an average score better than âˆ’60.

### Environment Roles

| Agent     | Capabilities |
|----------|--------------|
| **Speaker** | Can speak, but cannot move |
| **Listener** | Moves through the environment based on received messages |

---

## Algorithms

### MATD3 â€” Multi-Agent Twin Delayed DDPG
- Off-policy algorithm with replay buffer
- Extension of TD3 to multi-agent settings

### MADDPG â€” Multi-Agent Deep Deterministic Policy Gradient
- Extension of DDPG for multi-agent CTDE (Centralized Training, Decentralized Execution)

### IPPO â€” Independent Proximal Policy Optimization
- Each agent runs PPO independently
- On-policy algorithm with clipping for stable learning

---

## Experimental Setup

Six versions of the training pipeline were tested:

| Version | Algorithm | Description |
|--------|-----------|-------------|
| **V1** | MATD3 | Baseline from course setup with original networks/hyperparameters |
| **V2** | MATD3 | Smaller architecture + stronger exploration |
| **V3** | MATD3 | Stabilized learning with exploration decay |
| **V4** | MATD3 | Larger network + online hyperparameter adaptation |
| **V5** | MADDPG | Centralized-critic alternative |
| **V6** | IPPO | On-policy PPO-based independent learners |

---

## Version Details

### V1 â€” MATD3 Baseline
- Original configuration
- Serves as reference model

---

### V2 â€” MATD3 with Increased Exploration
**Architecture**
- Latent dim = 64
- 2Ã— 64-unit hidden layers

**Hyperparameters**
- Stronger exploration (`EXPL_NOISE = 0.2`)
- `GAMMA = 0.99`
- Smaller replay buffer
- More aggressive PBT exploration

---

### V3 â€” MATD3 with Stabilized Exploration
- Same network as V2
- Exploration decay (`0.1 â†’ 0.02`)
- `POLICY_FREQ = 1` (policy update every critic update)
- Conservative PBT mutations

---

### V4 â€” MATD3 High-Capacity + Online Hyperparameter Tuning ðŸ‘‘
- Latent dim = 128
- Larger network (128Ã—128 hidden layers)
- Learning-rate & policy update tuned online via PBT  
- **Best final score overall**

---

### V5 â€” MADDPG (Centralized Critic Alternative)
- High-capacity architecture (same as V4)
- Exploration noise decays during training
- Hyperparameters evolved via PBT

---

### V6 â€” IPPO (Fast On-Policy Coordination) ðŸš€
- Same architecture as V4/V5
- PPO-based updates w/ entropy & clipping
- **Fastest learning improvement**

---

## Visualization

Training results stored under `models/`.  
Plots generated by `training-plot.ipynb`, including:

- Target score line: **âˆ’60**
- Best score achieved per experiment
- Moving-average smoothing (50 steps)
- Combined performance comparison (**all_training_scores.png**)

---

## Results

| Metric | Winner | Notes |
|--------|--------|------|
| **Fastest improvement** | IPPO_V1 | Rapid on-policy updates |
| **Highest final score** | MATD3_V4 | Strong long-term convergence |

Additional findings:

âœ” 4 models surpass target score  
âœ” 3 models reach target before **50k** steps  
âœ” MATD3_V4 peak score â‰ˆ **âˆ’18.33** ðŸŽ¯  

---

## Summary

- Multi-agent RL performance varies significantly by algorithm
- **IPPO** adapts quickly early in training
- **MATD3_V4** achieves the strongest long-term results due to high capacity + dynamic HP tuning
- Population-Based Training (PBT) effectively improves both stability & exploration

---

## Conclusion

This project demonstrates that:

- Decentralized execution can achieve high coordination performance
- On-policy PPO is faster to improve in non-stationary environments
- Hyperparameter evolution is **critical** for performance in complex MARL settings

Future work may include:
- Curriculum learning for more difficult tasks
- Communication channel optimization
- Testing scalability with more agents

---
